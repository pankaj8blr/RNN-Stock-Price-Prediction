{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjf0S2yqD0uf"
      },
      "source": [
        "# Stock Price Prediction Using RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhNGhRQWjN94"
      },
      "source": [
        "## Objective\n",
        "The objective of this assignment is to try and predict the stock prices using historical data from four companies IBM (IBM), Google (GOOGL), Amazon (AMZN), and Microsoft (MSFT).\n",
        "\n",
        "We use four different companies because they belong to the same sector: Technology. Using data from all four companies may improve the performance of the model. This way, we can capture the broader market sentiment.\n",
        "\n",
        "The problem statement for this assignment can be summarised as follows:\n",
        "\n",
        "> Given the stock prices of Amazon, Google, IBM, and Microsoft for a set number of days, predict the stock price of these companies after that window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4gU_Gs--yjG"
      },
      "source": [
        "## Business Value\n",
        "\n",
        "Data related to stock markets lends itself well to modeling using RNNs due to its sequential nature. We can keep track of opening prices, closing prices, highest prices, and so on for a long period of time as these values are generated every working day. The patterns observed in this data can then be used to predict the future direction in which stock prices are expected to move. Analyzing this data can be interesting in itself, but it also has a financial incentive as accurate predictions can lead to massive profits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC424ieDIlaK"
      },
      "source": [
        "### **Data Description**\n",
        "\n",
        "You have been provided with four CSV files corresponding to four stocks: AMZN, GOOGL, IBM, and MSFT. The files contain historical data that were gathered from the websites of the stock markets where these companies are listed: NYSE and NASDAQ. The columns in all four files are identical. Let's take a look at them:\n",
        "\n",
        "- `Date`: The values in this column specify the date on which the values were recorded. In all four files, the dates range from Jaunary 1, 2006 to January 1, 2018.\n",
        "\n",
        "- `Open`: The values in this column specify the stock price on a given date when the stock market opens.\n",
        "\n",
        "- `High`: The values in this column specify the highest stock price achieved by a stock on a given date.\n",
        "\n",
        "- `Low`: The values in this column specify the lowest stock price achieved by a stock on a given date.\n",
        "\n",
        "- `Close`: The values in this column specify the stock price on a given date when the stock market closes.\n",
        "\n",
        "- `Volume`: The values in this column specify the total number of shares traded on a given date.\n",
        "\n",
        "- `Name`: This column gives the official name of the stock as used in the stock market.\n",
        "\n",
        "There are 3019 records in each data set. The file names are of the format `\\<company_name>_stock_data.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzV7hepM8K-H"
      },
      "source": [
        "## **1 Data Loading and Preparation** <font color =red> [25 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6UQ2zyxH1Ep"
      },
      "source": [
        "#### **Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RyZsIlDgfO3s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "31a78077-9127-4257-e103-4b54bca0eaeb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_HTMLDocumentationLinkMixin' from 'sklearn.utils._estimator_html_repr' (/usr/local/lib/python3.11/dist-packages/sklearn/utils/_estimator_html_repr.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-01d4dd0875eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_HTMLDocumentationLinkMixin' from 'sklearn.utils._estimator_html_repr' (/usr/local/lib/python3.11/dist-packages/sklearn/utils/_estimator_html_repr.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import libraries\n",
        "!pip install scikit-learn --upgrade\n",
        "!pip install scikeras --upgrade\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Fixed import: using the correct module path for train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "!pip install scikeras\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Set seaborn style\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4_Xnhwsl-00"
      },
      "source": [
        "### **1.1 Data Aggregation** <font color =red> [7 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5clrTkdAvq8G"
      },
      "source": [
        "As we are using the stock data for four different companies, we need to create a new DataFrame that contains the combined data from all four data frames. We will create a function that takes in a list of the file names for the four CSV files, and returns a single data frame. This function performs the following tasks:\n",
        "- Extract stock names from file names\n",
        "- Read the CSV files as data frames\n",
        "- Append the stock names into the columns of their respective data frames\n",
        "- Drop unnecessary columns\n",
        "- Join the data frames into one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tPMCwNwe1JS"
      },
      "source": [
        "#### **1.1.1** <font color =red> [5 marks] </font>\n",
        "Create the function to join DataFrames and use it to combine the four datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gj0K2Q65yix"
      },
      "outputs": [],
      "source": [
        "# Define a function to load data and aggregate them\n",
        "\n",
        "def combine_stock_data(file_list):\n",
        "    dataframes = []\n",
        "\n",
        "    for file_path in file_list:\n",
        "        # Extract stock name from filename\n",
        "        stock_name = os.path.basename(file_path).split('_')[0]\n",
        "\n",
        "        # Read the CSV\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Add a new column for the stock name\n",
        "        df['Stock'] = stock_name\n",
        "\n",
        "        # Optionally, drop unnecessary columns\n",
        "        # (assuming 'Unnamed: 0' or similar)\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "            df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "        # Append to list\n",
        "        dataframes.append(df)\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0sYevlt7kgn"
      },
      "outputs": [],
      "source": [
        "# Specify the names of the raw data files to be read and use the aggregation function to read the files\n",
        "\n",
        "# Example usage\n",
        "file_list = [\n",
        "    'https://raw.githubusercontent.com/pankaj8blr/RNN-Stock-Price-Prediction/2eabcaa2d00607cbbbd0cad05ae5d4b212c915e5/RNN_Stocks_Data/AMZN_stocks_data.csv',\n",
        "    'https://raw.githubusercontent.com/pankaj8blr/RNN-Stock-Price-Prediction/2eabcaa2d00607cbbbd0cad05ae5d4b212c915e5/RNN_Stocks_Data/GOOGL_stocks_data.csv',\n",
        "    'https://raw.githubusercontent.com/pankaj8blr/RNN-Stock-Price-Prediction/2eabcaa2d00607cbbbd0cad05ae5d4b212c915e5/RNN_Stocks_Data/IBM_stocks_data.csv',\n",
        "    'https://raw.githubusercontent.com/pankaj8blr/RNN-Stock-Price-Prediction/2eabcaa2d00607cbbbd0cad05ae5d4b212c915e5/RNN_Stocks_Data/MSFT_stocks_data.csv'\n",
        "]\n",
        "\n",
        "combined_data = combine_stock_data(file_list)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(combined_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTpnaj6MCHN"
      },
      "source": [
        "### View specifics of the data ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qMKdhGj8cKF"
      },
      "outputs": [],
      "source": [
        "# View specifics of the data\n",
        "print(f\"Shape of combined data: {combined_data.shape}\")\n",
        "\n",
        "\n",
        "print(\"Column names:\")\n",
        "print(combined_data.columns.tolist())\n",
        "\n",
        "\n",
        "print(\"First 5 rows:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "\n",
        "print(\"Data types and missing values:\")\n",
        "print(combined_data.info())\n",
        "\n",
        "\n",
        "print(\"Statistical Summary:\")\n",
        "print(combined_data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDau5WlqMCHN"
      },
      "source": [
        "#### Summary ####\n",
        "- Shape: 12,077 rows × 8 columns\n",
        "\n",
        "- Columns:\n",
        "    - Date (object/string)\n",
        "    - Open (float)\n",
        "    - High (float)\n",
        "    - Low (float)\n",
        "    - Close (float)\n",
        "    - Volume (int)\n",
        "    - Name (object/string) — seems to repeat the stock name\n",
        "    - Stock (object/string) — the one we added for stock identity\n",
        "- Data Types & Nulls:\n",
        "    - No major missing data (Open and Low have only 1 missing value out of 12077 rows).\n",
        "    - All numeric columns are floats/ints.\n",
        "    - Date, Name, and Stock are strings (object dtype).\n",
        "- Observations:\n",
        "    - Stock prices range widely (from ~$15 up to $1200+), which makes sense because you have  companies like Amazon and Google here.\n",
        "    - Volume is huge for some days — up to nearly 600 million"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwVzhIuBfRGn"
      },
      "source": [
        "#### **1.1.2** <font color =red> [2 marks] </font>\n",
        "Identify and handle any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vmQGhf69x36"
      },
      "outputs": [],
      "source": [
        "# Handle Missing Values\n",
        "\n",
        "combined_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeK6FIfJMCHO"
      },
      "outputs": [],
      "source": [
        "combined_data.fillna(method='ffill')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuFFxr5jQ4xw"
      },
      "source": [
        "### **1.2 Analysis and Visualisation** <font color =red> [5 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnybVo_YQ4xx"
      },
      "source": [
        "#### **1.2.1** <font color =red> [2 marks] </font>\n",
        "Analyse the frequency distribution of stock volumes of the companies and also see how the volumes vary over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_wD-3h6MCHP"
      },
      "outputs": [],
      "source": [
        "# Convert 'Date' to datetime for time-based plots\n",
        "combined_data['Date'] = pd.to_datetime(combined_data['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGxyKbfRcFl-"
      },
      "outputs": [],
      "source": [
        "# Frequency distribution of volumes\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(combined_data['Volume'], bins=50, kde=True, color='skyblue')\n",
        "plt.title('Frequency Distribution of Stock Volumes')\n",
        "plt.xlabel('Volume')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZOfTkWqMCHQ"
      },
      "outputs": [],
      "source": [
        "unique_stocks = combined_data['Stock'].unique()\n",
        "num_stocks = len(unique_stocks)\n",
        "\n",
        "fig, axs = plt.subplots(num_stocks, 1, figsize=(14, num_stocks*4), sharex=True)\n",
        "\n",
        "for i, stock in enumerate(unique_stocks):\n",
        "    stock_data = combined_data[combined_data['Stock'] == stock]\n",
        "    axs[i].plot(stock_data['Date'], stock_data['Volume'], label=stock, color='steelblue')\n",
        "    axs[i].set_title(f'{stock} Volume Over Time')\n",
        "    axs[i].set_ylabel('Volume')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXMJvdnGMCHQ"
      },
      "outputs": [],
      "source": [
        "g = sns.FacetGrid(combined_data, col=\"Stock\", col_wrap=2, height=4, aspect=1.5)\n",
        "g.map_dataframe(sns.lineplot, x=\"Date\", y=\"Volume\", color=\"steelblue\")\n",
        "g.set_axis_labels(\"Date\", \"Volume\")\n",
        "g.set_titles(\"{col_name} Stock\")\n",
        "g.fig.suptitle('Stock Volume Variation Over Time (FacetGrid)', fontsize=16, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q1g9Y0JcNW0"
      },
      "outputs": [],
      "source": [
        "# Stock volume variation over time\n",
        "plt.figure(figsize=(14,8))\n",
        "for stock in combined_data['Stock'].unique():\n",
        "    stock_data = combined_data[combined_data['Stock'] == stock]\n",
        "    plt.plot(stock_data['Date'], stock_data['Volume'], label=stock)\n",
        "\n",
        "plt.title('Stock Volume Variation Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO5frnjURc5i"
      },
      "source": [
        "#### **1.2.2** <font color =red> [3 marks] </font>\n",
        "Analyse correlations between features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE9ijOnmP94M"
      },
      "outputs": [],
      "source": [
        "# Analyse correlations\n",
        "\n",
        "correlation_matrix = combined_data[['Open', 'High', 'Low', 'Close', 'Volume']].corr()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgWgmFdlMCHR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Stock Features')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Display correlation matrix as text\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1dHdCDQl-1K"
      },
      "source": [
        "### **1.3 Data Processing** <font color =red> [13 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjjEYTmOopW4"
      },
      "source": [
        "Next, we need to process the data so that it is ready to be used in recurrent neural networks. You know RNNs are suitable to work with sequential data where patterns repeat at regular intervals.\n",
        "\n",
        "For this, we need to execute the following steps:\n",
        "1. Create windows from the master data frame and obtain windowed `X` and corresponding windowed `y` values\n",
        "2. Perform train-test split on the windowed data\n",
        "3. Scale the data sets in an appropriate manner\n",
        "\n",
        "We will define functions for the above steps that finally return training and testing data sets that are ready to be used in recurrent neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTLu14Wuid5n"
      },
      "source": [
        "**Hint:** If we use a window of size 3, in the first window, the rows `[0, 1, 2]` will be present and will be used to predict the value of `CloseAMZN` in row `3`. In the second window, rows `[1, 2, 3]` will be used to predict `CloseAMZN` in row `4`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mulMxtd6fsFR"
      },
      "source": [
        "#### **1.3.1** <font color =red> [3 marks] </font>\n",
        "Create a function that returns the windowed `X` and `y` data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXQOh3iP6W7L"
      },
      "source": [
        "From the main DataFrame, this function will create windowed DataFrames, and store those as a list of DataFrames.\n",
        "\n",
        "Controllable parameters will be window size, step size (window stride length) and target names as a list of the names of stocks whose closing values we wish to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n1M1-4fvq8i"
      },
      "outputs": [],
      "source": [
        "# Define a function that divides the data into windows and generates target variable values for each window\n",
        "\n",
        "def create_windowed_data(df, stock_names, window_size=10, stride=1):\n",
        "    \"\"\"\n",
        "    Creates windowed X and y for selected stocks.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Master combined stock data\n",
        "        stock_names (list): List of stock names to predict\n",
        "        window_size (int): Length of each input window\n",
        "        stride (int): Stride to slide windows\n",
        "    Returns:\n",
        "        X, y (both numpy arrays)\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Filter for selected stocks\n",
        "    df_selected = df[df['Stock'].isin(stock_names)]\n",
        "\n",
        "    # Sort by Date to maintain temporal order\n",
        "    df_selected = df_selected.sort_values(['Stock', 'Date'])\n",
        "\n",
        "    # Group by Stock so that windows don't mix companies\n",
        "    for stock, group in df_selected.groupby('Stock'):\n",
        "        features = group[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
        "        target = group['Close'].values\n",
        "\n",
        "        for i in range(0, len(features) - window_size, stride):\n",
        "            X.append(features[i:i+window_size])\n",
        "            y.append(target[i+window_size])  # Target is the Close price *after* the window\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw5JL8tgmwc-"
      },
      "source": [
        "#### **1.3.2** <font color =red> [3 marks] </font>\n",
        "Create a function to scale the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBaJkaIx_1Vg"
      },
      "source": [
        "Define a function that will scale the data.\n",
        "\n",
        "For scaling, we have to look at the whole length of data to find max/min values or standard deviations and means. If we scale the whole data at once, this will lead to data leakage in the windows. This is not necessarily a problem if the model is trained on the complete data with cross-validation.\n",
        "\n",
        "One way to scale when dealing with windowed data is to use the `partial_fit()` method.\n",
        "```\n",
        "scaler.partial_fit(window)\n",
        "scaler.transform(window)\n",
        "```\n",
        "You may use any other suitable way to scale the data properly. Arrive at a reasonable way to scale your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9586NZptrAi"
      },
      "outputs": [],
      "source": [
        "# Define a function that scales the windowed data\n",
        "# The function takes in the windowed data sets and returns the scaled windows\n",
        "\n",
        "def split_train_test(X, y, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Splits X and y into training and testing sets.\n",
        "    \"\"\"\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42, shuffle=False)  # No shuffle because it's time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3HhUS59DaCQ"
      },
      "source": [
        "Next, define the main function that will call the windowing and scaling helper functions.\n",
        "\n",
        "The input parameters for this function are:\n",
        "- The joined master data set\n",
        "- The names of the stocks that we wish to predict the *Close* prices for\n",
        "- The window size\n",
        "- The window stride\n",
        "- The train-test split ratio\n",
        "\n",
        "The outputs from this function are the scaled dataframes:\n",
        "- *X_train*\n",
        "- *y_train*\n",
        "- *X_test*\n",
        "- *y_test*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tND37dF1ksmy"
      },
      "source": [
        "#### **1.3.3** <font color =red> [3 marks] </font>\n",
        "Define a function to create windows of `window_size` and split the windowed data in to training and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3acwSBoArWU"
      },
      "source": [
        "The function can take arguments such as list of target names, window size, window stride and split ratio. Use the windowing function here to make windows in the data and then perform scaling and train-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvWRH5J4vq8l"
      },
      "outputs": [],
      "source": [
        "# Define a function to create input and output data points from the master DataFrame\n",
        "\n",
        "# 2. Train-Test Split\n",
        "def split_train_test(X, y, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Splits X and y into training and testing sets.\n",
        "    \"\"\"\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42, shuffle=False)  # No shuffle because it's time series\n",
        "\n",
        "# 3. Scaling Function\n",
        "def scale_windowed_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Scales windowed data feature-wise.\n",
        "\n",
        "    Each feature across all windows and timesteps is scaled separately.\n",
        "    \"\"\"\n",
        "    n_timesteps = X_train.shape[1]\n",
        "    n_features = X_train.shape[2]\n",
        "\n",
        "    # Reshape to (samples * timesteps, features) for scaling\n",
        "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
        "    X_test_reshaped = X_test.reshape(-1, n_features)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
        "    X_test_scaled = scaler.transform(X_test_reshaped)\n",
        "\n",
        "    # Reshape back to original 3D form\n",
        "    X_train_scaled = X_train_scaled.reshape(-1, n_timesteps, n_features)\n",
        "    X_test_scaled = X_test_scaled.reshape(-1, n_timesteps, n_features)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, scaler\n",
        "\n",
        "# 4. Main Wrapper Function\n",
        "def prepare_windowed_data(df, stock_names, window_size=10, stride=1, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Full pipeline: Windowing, Train-Test Split, Scaling.\n",
        "\n",
        "    Returns:\n",
        "        X_train_scaled, y_train, X_test_scaled, y_test\n",
        "    \"\"\"\n",
        "    # Step 1: Window the data\n",
        "    X, y = create_windowed_data(df, stock_names, window_size, stride)\n",
        "\n",
        "    # Step 2: Train-test split\n",
        "    X_train, X_test, y_train, y_test = split_train_test(X, y, test_size)\n",
        "\n",
        "    # Step 3: Scale the data\n",
        "    X_train_scaled, X_test_scaled, scaler = scale_windowed_data(X_train, X_test)\n",
        "\n",
        "    return X_train_scaled, y_train, X_test_scaled, y_test, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHSBYQGTMCHc"
      },
      "outputs": [],
      "source": [
        "stock_list = ['AMZN', 'GOOGL', 'IBM', 'MSFT']\n",
        "window_size = 20\n",
        "stride = 1\n",
        "test_size = 0.2\n",
        "\n",
        "X_train, y_train, X_test, y_test, scaler = prepare_windowed_data(\n",
        "    combined_data,\n",
        "    stock_list,\n",
        "    window_size=window_size,\n",
        "    stride=stride,\n",
        "    test_size=test_size\n",
        ")\n",
        "\n",
        "print(f\"Train X shape: {X_train.shape}\")\n",
        "print(f\"Train y shape: {y_train.shape}\")\n",
        "print(f\"Test X shape: {X_test.shape}\")\n",
        "print(f\"Test y shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5PZw7SHGUdx"
      },
      "source": [
        "We can now use these helper functions to create our training and testing data sets. But first we need to decide on a length of windows. As we are doing time series prediction, we want to pick a sequence that shows some repetition of patterns.\n",
        "\n",
        "For selecting a good sequence length, some business understanding will help us. In financial scenarios, we can either work with business days, weeks (which comprise of 5 working days), months, or quarters (comprising of 13 business weeks). Try looking for some patterns for these periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mihb_duwxWeC"
      },
      "source": [
        "#### **1.3.4** <font color =red> [2 marks] </font>\n",
        "Identify an appropriate window size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG-LrkMVjgT-"
      },
      "source": [
        "For this, you can use plots to see how target variable is varying with time. Try dividing it into parts by weeks/months/quarters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhhbomJIrlK_"
      },
      "outputs": [],
      "source": [
        "# Checking for patterns in different sequence lengths\n",
        "# Make sure 'Date' is datetime\n",
        "combined_data['Date'] = pd.to_datetime(combined_data['Date'])\n",
        "\n",
        "# Pick one stock to analyze patterns (e.g., AMZN)\n",
        "# stock_to_analyze = file_list[0]\n",
        "stock_to_analyze = 'AMZN'\n",
        "stock_df = combined_data[combined_data['Stock'] == stock_to_analyze].copy()\n",
        "stock_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Resampling to Weekly, Monthly, Quarterly Close Price\n",
        "resampled = {\n",
        "    'Daily': stock_df['Close'],\n",
        "    'Weekly': stock_df['Close'].resample('W').mean(),\n",
        "    'Monthly': stock_df['Close'].resample('M').mean(),\n",
        "    'Quarterly': stock_df['Close'].resample('Q').mean()\n",
        "}\n",
        "\n",
        "# Plot all resolutions\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
        "\n",
        "for i, (freq, series) in enumerate(resampled.items()):\n",
        "    axes[i].plot(series.index, series.values, label=f'{freq} Close Price', color='tab:blue')\n",
        "    axes[i].set_title(f'{stock_to_analyze} Close Price - {freq}')\n",
        "    axes[i].set_ylabel('Close')\n",
        "    axes[i].legend(loc='upper left')\n",
        "\n",
        "axes[-1].set_xlabel('Date')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZpKaENfyblo"
      },
      "source": [
        "#### **1.3.5** <font color =red> [2 marks] </font>\n",
        "Call the functions to create testing and training instances of predictor and target features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sFdDgH0vq8o"
      },
      "outputs": [],
      "source": [
        "# Create data instances from the master data frame using decided window size and window stride\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create windowed data function\n",
        "def create_windowed_data_instances(df, stock_names, window_size=30, stride=1):\n",
        "    \"\"\"\n",
        "    Create data instances (X, y) from master DataFrame using window size and stride.\n",
        "\n",
        "    Parameters:\n",
        "    - df: master stock dataframe\n",
        "    - stock_names: list of stock names to consider\n",
        "    - window_size: number of timesteps in each input window\n",
        "    - stride: how much to slide the window forward\n",
        "\n",
        "    Returns:\n",
        "    - X: numpy array of windowed input sequences\n",
        "    - y: numpy array of target close prices\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Filter only the required stocks\n",
        "    df_filtered = df[df['Stock'].isin(stock_names)].copy()\n",
        "\n",
        "    # Sort data properly\n",
        "    df_filtered = df_filtered.sort_values(by=['Stock', 'Date'])\n",
        "\n",
        "    # Group by stock so windows don't cross companies\n",
        "    for stock, stock_df in df_filtered.groupby('Stock'):\n",
        "        features = stock_df[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
        "        target = stock_df['Close'].values\n",
        "\n",
        "        for i in range(0, len(features) - window_size, stride):\n",
        "            X.append(features[i:i+window_size])\n",
        "            y.append(target[i+window_size])  # Target is the Close price after the window\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -----------------------------------\n",
        "\n",
        "# Example: Create instances\n",
        "chosen_stocks = ['AMZN', 'GOOGL', 'IBM', 'MSFT']\n",
        "window_size = 30  # 30 days window\n",
        "stride = 1  # move window 1 day at a time\n",
        "\n",
        "X, y = create_windowed_data_instances(combined_data, chosen_stocks, window_size, stride)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")  # (samples, window_size, features)\n",
        "print(f\"y shape: {y.shape}\")  # (samples,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoRLGqp-6aHP"
      },
      "outputs": [],
      "source": [
        "# Check the number of data points generated\n",
        "print(f\"Number of X (input) instances: {X.shape[0]}\")\n",
        "print(f\"Number of y (target) instances: {y.shape[0]}\")\n",
        "\n",
        "# Double-check that X and y match\n",
        "assert X.shape[0] == y.shape[0], \"Mismatch between X and y instances!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51IV5zmjBf-w"
      },
      "source": [
        "**Check if the training and testing datasets are in the proper format to feed into neural networks.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViVKgkrwvq8r"
      },
      "outputs": [],
      "source": [
        "# Check if the datasets are compatible inputs to neural networks\n",
        "\n",
        "# 1. Check shapes\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# 2. Check missing values\n",
        "print(f\"Any NaN in X? {np.isnan(X).any()}\")\n",
        "print(f\"Any NaN in y? {np.isnan(y).any()}\")\n",
        "\n",
        "# 3. Check data types\n",
        "print(f\"X dtype: {X.dtype}\")\n",
        "print(f\"y dtype: {y.dtype}\")\n",
        "\n",
        "# 4. Optional: Ensure y is 2D (reshaped if needed)\n",
        "if len(y.shape) == 1:\n",
        "    y = y.reshape(-1, 1)\n",
        "    print(f\"y reshaped to: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNNSAT6YfO3z"
      },
      "source": [
        "## **2 RNN Models** <font color =red> [20 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqHs0PjzwCve"
      },
      "source": [
        "In this section, we will:\n",
        "- Define a function that creates a simple RNN\n",
        "- Tune the RNN for different hyperparameter values\n",
        "- View the performance of the optimal model on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekq-LY1p86NI"
      },
      "source": [
        "### **2.1 Simple RNN Model** <font color =red> [10 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdzl5ojcyDX7"
      },
      "source": [
        "#### **2.1.1** <font color =red> [3 marks] </font>\n",
        "Create a function that builds a simple RNN model based on the layer configuration provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owGKpgMxRtk2"
      },
      "outputs": [],
      "source": [
        "# Create a function that creates a simple RNN model according to the model configuration arguments\n",
        "\n",
        "def create_simple_rnn_model(input_shape,\n",
        "                             rnn_units=50,\n",
        "                             dense_units=1,\n",
        "                             dropout_rate=0.2,\n",
        "                             optimizer='adam',\n",
        "                             loss='mse'):\n",
        "    \"\"\"\n",
        "    Function to create a simple RNN model with configurable arguments.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: tuple, shape of input (timesteps, features)\n",
        "    - rnn_units: int, number of units in the RNN layer\n",
        "    - dense_units: int, number of output units (default 1 for regression)\n",
        "    - dropout_rate: float, dropout rate after RNN\n",
        "    - optimizer: optimizer to use\n",
        "    - loss: loss function to use\n",
        "\n",
        "    Returns:\n",
        "    - model: compiled RNN model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # RNN layer\n",
        "    model.add(SimpleRNN(units=rnn_units, input_shape=input_shape, return_sequences=False))\n",
        "\n",
        "    # Dropout layer\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    # Dense output layer\n",
        "    model.add(Dense(units=dense_units))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4xDFvOXfO31"
      },
      "source": [
        "#### **2.1.2** <font color =red> [4 marks] </font>\n",
        "Perform hyperparameter tuning to find the optimal network configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGRsV3GefO31",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Find an optimal configuration of simple RNN\n",
        "\n",
        "# def build_rnn_model(rnn_units=50, dropout_rate=0.2, optimizer='adam'):\n",
        "#     model = Sequential()\n",
        "#     model.add(SimpleRNN(units=rnn_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "#     model.add(Dropout(rate=dropout_rate))\n",
        "#     model.add(Dense(1))  # Regression output\n",
        "#     model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "#     return model\n",
        "# regressor = KerasRegressor(build_fn=build_rnn_model, verbose=0)\n",
        "# param_grid = {\n",
        "#     'rnn_units': [32, 64],\n",
        "#     'dropout_rate': [0.2, 0.3],\n",
        "#     'optimizer': ['adam'],\n",
        "#     'batch_size': [32],\n",
        "#     'epochs': [10]  # Increase later for final model\n",
        "# }\n",
        "# grid = GridSearchCV(estimator=regressor,\n",
        "#                     param_grid=param_grid,\n",
        "#                     scoring='neg_mean_squared_error',\n",
        "#                     cv=3,\n",
        "#                     verbose=1,\n",
        "#                     n_jobs=-1)\n",
        "\n",
        "# grid_result = grid.fit(X_train, y_train)\n",
        "# print(\"Best Score (Neg. MSE):\", grid_result.best_score_)\n",
        "# print(\"Best Parameters:\", grid_result.best_params_)\n",
        "\n",
        "# for mean, std, params in zip(grid_result.cv_results_['mean_test_score'],\n",
        "#                              grid_result.cv_results_['std_test_score'],\n",
        "#                              grid_result.cv_results_['params']):\n",
        "#     print(f\"{mean:.4f} ± {std:.4f} for {params}\")\n",
        "\n",
        "# !pip install scikit-learn==1.3.0 --upgrade\n",
        "# !pip install scikeras --upgrade\n",
        "# Find an optimal configuration of simple RNN\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_rnn_model(rnn_units=50, dropout_rate=0.2, optimizer='adam'):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(units=rnn_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))  # Regression output\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "regressor = KerasRegressor(model=build_rnn_model, verbose=0) # Changed build_fn to model\n",
        "\n",
        "param_grid = {\n",
        "    'rnn_units': [32, 64],\n",
        "    'dropout_rate': [0.2, 0.3],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [10]  # Increase later for final model\n",
        "}\n",
        "grid = GridSearchCV(estimator=regressor,\n",
        "                    param_grid=param_grid,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    cv=3,\n",
        "                    verbose=1,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "print(\"Best Score (Neg. MSE):\", grid_result.best_score_)\n",
        "print(\"Best Parameters:\", grid_result.best_params_)\n",
        "\n",
        "for mean, std, params in zip(grid_result.cv_results_['mean_test_score'],\n",
        "                             grid_result.cv_results_['std_test_score'],\n",
        "                             grid_result.cv_results_['params']):\n",
        "    print(f\"{mean:.4f} ± {std:.4f} for {params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHuJ61iYfO31"
      },
      "outputs": [],
      "source": [
        "# Find the best configuration based on evaluation metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8krOJTq5_fM0"
      },
      "source": [
        "#### **2.1.3** <font color =red> [3 marks] </font>\n",
        "Run for optimal Simple RNN Model and show final results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLQPoIQCfO32",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QceWx7D78RH"
      },
      "source": [
        "Plotting the actual vs predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1yY499Ynpq"
      },
      "outputs": [],
      "source": [
        "# Predict on the test data and plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GUfu3167Rqx"
      },
      "source": [
        "It is worth noting that every training session for a neural network is unique. So, the results may vary slightly each time you retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHlPLvDcfO32"
      },
      "outputs": [],
      "source": [
        "# Compute the performance of the model on the testing data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLmMGXvyvq9V"
      },
      "source": [
        "### **2.2 Advanced RNN Models** <font color =red> [10 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaJYIHzwzP9"
      },
      "source": [
        "In this section, we will:\n",
        "- Create an LSTM or a GRU network\n",
        "- Tune the network for different hyperparameter values\n",
        "- View the performance of the optimal model on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6KBxY4Jasm5"
      },
      "source": [
        "#### **2.2.1** <font color =red> [3 marks] </font>\n",
        "Create a function that builds an advanced RNN model with tunable hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g1yDzllvq9W"
      },
      "outputs": [],
      "source": [
        "# # Define a function to create a model and specify default values for hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0lvqIJ2vq9b"
      },
      "source": [
        "#### **2.2.2** <font color =red> [4 marks] </font>\n",
        "Perform hyperparameter tuning to find the optimal network configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWRBShecvq9e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Find an optimal configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n5OTBdSvq9t"
      },
      "source": [
        "#### **2.2.3** <font color =red> [3 marks] </font>\n",
        "Run for optimal RNN Model and show final results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uyN6vgzvq9w",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create the model with a combination of potentially optimal hyperparameter values and retrain the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x9njgzwvq92"
      },
      "outputs": [],
      "source": [
        "# Compute the performance of the model on the testing data set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Hy584ffhsS"
      },
      "source": [
        "Plotting the actual vs predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31CshI-SfhsS"
      },
      "outputs": [],
      "source": [
        "# Predict on the test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EStsPXCYf3_q"
      },
      "source": [
        "## **3 Predicting Multiple Target Variables** <font color =red> [OPTIONAL] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKaiTII-23Aq"
      },
      "source": [
        "In this section, we will use recurrent neural networks to predict stock prices for more than one company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taBw30-HLTWV"
      },
      "source": [
        "### **3.1 Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9aHdR6EgVqT"
      },
      "source": [
        "#### **3.1.1**\n",
        "Create testing and training instances for multiple target features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua3P_ySxgq9Z"
      },
      "source": [
        "You can take the closing price of all four companies to predict here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQEWm-m129Rw"
      },
      "outputs": [],
      "source": [
        "# Create data instances from the master data frame using a window size of 65, a window stride of 5 and a test size of 20%\n",
        "# Specify the list of stock names whose 'Close' values you wish to predict using the 'target_names' parameter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WWJKvsLlAA"
      },
      "outputs": [],
      "source": [
        "# Check the number of data points generated\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kUyl2U9LnpK"
      },
      "source": [
        "### **3.2 Run RNN Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2HTWIcWhLE0"
      },
      "source": [
        "#### **3.2.1**\n",
        "Perform hyperparameter tuning to find the optimal network configuration for Simple RNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhjXH72pMI4t",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Find an optimal configuration of simple RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYXngNUmMI4u"
      },
      "outputs": [],
      "source": [
        "# Find the best configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vyWykGBMI4y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j63wDQkMcNB"
      },
      "outputs": [],
      "source": [
        "# Compute the performance of the model on the testing data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QbM-hR6h-SV"
      },
      "outputs": [],
      "source": [
        "# Plotting the actual vs predicted values for all targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnwOVGv5MjXi"
      },
      "source": [
        "#### **3.2.2**\n",
        "Perform hyperparameter tuning to find the optimal network configuration for Advanced RNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQRqZZbEIrh9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Find an optimal configuration of advanced RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZswZ55JIrh-"
      },
      "outputs": [],
      "source": [
        "# Find the best configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fh-2tBTNWXI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a model with a combination of potentially optimal hyperparameter values and retrain the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y7m-nziNWXK"
      },
      "outputs": [],
      "source": [
        "# Compute the performance of the model on the testing data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OMsd4hHicRO"
      },
      "outputs": [],
      "source": [
        "# Plotting the actual vs predicted values for all targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YItIF9_SmeCN"
      },
      "source": [
        "## **4 Conclusion** <font color =red> [5 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGwEyQn1meCN"
      },
      "source": [
        "### **4.1 Conclusion and insights** <font color =red> [5 marks] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqBneTdDaVYw"
      },
      "source": [
        "#### **4.1.1** <font color =red> [5 marks] </font>\n",
        "Conclude with the insights drawn and final outcomes and results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}